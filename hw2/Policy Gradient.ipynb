{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL - Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the policy gradient algorithm applied on several gym environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "import gym\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the DNN for Policy, Value fn Estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the computation graph for DNN, which can be used as our policy as well as the value function estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_placeholder, output_size, scope, n_layers, size, activation=None, output_activation=None):\n",
    "    \"\"\"\n",
    "    Builds a feed foreward Neural Network\n",
    "    \n",
    "    Arguments:\n",
    "        input_placeholder - placeholder for the state observation - [batch_size, input_size]\n",
    "        output_size - size of the output place holder\n",
    "        scope - variable scope for the computation graph.\n",
    "        n_layers - number of layers in the NN\n",
    "        size - no of neurons in the hidden layers\n",
    "        activatio_fn - activation function to be used in hidden layers.\n",
    "        output_activation - activation function to be used in the output layer.\n",
    "    Returns:\n",
    "        output_placeholder - result of a forward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        layer = input_placeholder\n",
    "        for n in range(n_layers):\n",
    "            layer = tf.layers.dense(layer, size, activation=activation)\n",
    "            \n",
    "        output_placeholder = tf.layers.dense(layer, output_size)\n",
    "        \n",
    "    return output_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL agent that interacts with the environment. collect experiences and improve itself to maximize the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, computation_graph_args, sample_trajectory_args, estimate_return_args):\n",
    "        \"\"\"\n",
    "        computation_graph_args: params used to define the policy/value fn approximator computatin graph\n",
    "        sample_trajectory_args : params used to sample trajectory from the environment.\n",
    "        estimate_return_rgs : params used to estimate the returns/rewards of the sampled trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        # Computation Graph Params\n",
    "        self.ob_dim = computation_graph_args['ob_dim']\n",
    "        self.ac_dim = computation_graph_args['ac_dim']\n",
    "        self.discrete = computation_graph_args['discrete']\n",
    "        self.size = computation_graph_args['size']\n",
    "        self.n_layers = computation_graph_args['n_layer']\n",
    "        self.learning_rate = computation_graph_args['learning_rate']\n",
    "        \n",
    "        # Sample Trajectory Params\n",
    "        self.animate = sample_trajectory_args['animate']\n",
    "        self.max_length_path = sample_trajectory_args['max_path_length']\n",
    "        self.min_timsteps_per_batch = sample_trajectory_args['min_timesteps_per_batch']\n",
    "        \n",
    "        # Estimate returns params\n",
    "        self.gamma = estimate_return_args['gamma']\n",
    "        self.reward_to_go = estimate_return_args['reward_to_go']\n",
    "        self.nn_baseline = estimate_return_args['nn_baseline']\n",
    "        self.normalize_advantages = estimate_return_args['normalize_advantages']\n",
    "        \n",
    "        \n",
    "    def init_tf_sess(self):\n",
    "        \n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        \n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "        placeholders for the inputs such as batch observations, actions and advantages in policy gradient loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        sy_ob_no = tf.placeholder(dtype=tf.float32, shape=[None, self.ob_dim], name='observation')\n",
    "        \n",
    "        if self.discrete:\n",
    "            sy_ac_na = tf.placeholder(dtype=tf.int32, shape=[None], name='action')\n",
    "        else:\n",
    "            sy_ac_na = tf.placeholder(dtype=tf.float32, shap=e[None, self.ac_dim], name='actions')\n",
    "        \n",
    "        sy_adv_n = tf.placeholder(dtype=tf.float32, shape=[None], name='advantage')\n",
    "        \n",
    "        return sy_ob_no, sy_ac_na, sy_adv_n\n",
    "    \n",
    "    def policy_forward_pass(self, sy_ob_no):\n",
    "        \"\"\"\n",
    "        Construct the symbolic operations/computation graph for the policy network outputs. which are the parameters of the\n",
    "        policy distribution p(a|s)\n",
    "        \n",
    "        arguments:\n",
    "            sy_ob_no: (batch_size, self.ob_dim)\n",
    "            \n",
    "        returns:\n",
    "            the parameters of the policy\n",
    "            \n",
    "            if discrete, the parameters are the logits of a categorical distribution over the action\n",
    "            \n",
    "                sy_logits_na: (batchsize, self.ac_dim)\n",
    "            \n",
    "            if continous, the parameters are a tuple (mean, log_std) of a Guassian\n",
    "                distribution over actions, log_std should be a trainable variable.\n",
    "            \n",
    "        \"\"\"\n",
    "        variable_scope = 'nn_policy'\n",
    "        if self.discrete:\n",
    "            sy_logits_na = build_mlp(sy_ob_no, self.ac_dim, variable_scope, self.n_layers, self.size, activation=tf.nn.relu, output_activation=None)\n",
    "            return sy_logits_na\n",
    "        else:\n",
    "            sy_mean = build_mlp(sy_ob_no, self.ac_dim, variable_scope, self.n_layers, self.size, activation=tf.nn.relu, output_activatoin=None)\n",
    "            sy_logstd = tf.Variable(tf.zeros(self.ac_dim), name='sy_logstd')\n",
    "            return sy_mean, sy_logstd\n",
    "        \n",
    "        \n",
    "    def sample_action(self, policy_parameters):\n",
    "        \"\"\"\n",
    "        Constructs a symbolic operation for stochastically sampling from the policy distribution. \n",
    "        \n",
    "        arguments:\n",
    "            policy_parameters:\n",
    "                if discrete, logits of categorical distribution over actions\n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continous, (mean, log_std) of Guassian distribution over actions.\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim, )\n",
    "                    \n",
    "        returns:\n",
    "            sy_sampled_ac:\n",
    "                if discrete, (batch_size)\n",
    "                if continous, (batch_size, self.ac_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope('sampled_action'):\n",
    "            if self.discrete:\n",
    "                sy_logits_na = policy_parameters\n",
    "                sy_sampled_ac = tf.squeeze(tf.multinomial(sy_logits_na, 1), axis=1)\n",
    "            else:\n",
    "                sy_mean, sy_logstd = policy_parameters\n",
    "                sy_sampled_ac = sy_mean + tf.exp(sy_logstd) * tf.random_normal(shape=tf.shape(sy_mean))\n",
    "                \n",
    "        return sy_sampled_ac\n",
    "    \n",
    "    \n",
    "    def get_log_prob(self, policy_parameters, sy_ac_na):\n",
    "        \"\"\"\n",
    "        Constructs a symbolic operation for computing the log probabilities of a set of actions that were actually taken\n",
    "        according to the policy\n",
    "        \n",
    "        arguments:\n",
    "            policy_parameters\n",
    "                if discrete, logits of categorical distribution over actions\n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continours, (mean, log_std) of a Guassian distribution over actions.\n",
    "                    sy_mean: (batc_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "            sy_ac_na:\n",
    "                if discrete: (batch_size, )\n",
    "                if continous: (batch_size, self.ac_dim)\n",
    "                    \n",
    "        returns:\n",
    "            sy_logprob_n: (batchsize)\n",
    "            \n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            sy_logprob_n = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sy_logits_na, labels=sy_ac_na)\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            probabilities = tf.distributions.Normal(sy_mean, tf.exp(sy_logstdg)).prob(sy_ac_na)\n",
    "            sy_logprob_n = tf.log(tf.reduce_prod(probabilities, axis=1))\n",
    "        \n",
    "        return sy_logprob_n\n",
    "    \n",
    "    def build_computation_graph(self):\n",
    "        \"\"\"\n",
    "        build full computation graph of the training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # input placeholders\n",
    "        self.sy_ob_no, self.sy_ac_na, self.sy_adv_n = self.define_placeholders()\n",
    "        \n",
    "        # the policy takes in an observation and produces a distribution over actions\n",
    "        self.policy_parameters = self.policy_forward_pass(self.sy_ob_no)\n",
    "        \n",
    "        # We can sample action from this action distribution\n",
    "        # this is used in Agent.sample_trajectory() where we generate a rollout\n",
    "        self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "        \n",
    "        # We can also compute the log probability of actions that were actually taken by the policy\n",
    "        # This is used in loss function\n",
    "        self.sy_logprob_n = self.get_log_prob(self.policy_parameters, self.sy_ac_na)\n",
    "        \n",
    "        # THE LOSS FUNCTION\n",
    "        with tf.variable_scope(\"log_probability_weighted_by_advantage\"):\n",
    "            sy_weighted_logprob_n = tf.multiply(self.sy_logprob_n, self.sy_adv_n)\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.sy_loss = tf.reduce_mean(sy_weighted_logprob_n)\n",
    "            \n",
    "        self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.sy_loss)\n",
    "        \n",
    "        # In case of baseline estimation using another neural network\n",
    "        if self.nn_baseline:\n",
    "            \n",
    "            self.baseline_prediction = tf.squeeze(build_mlp(\n",
    "                                        self.sy_ob_no,\n",
    "                                        1,\n",
    "                                        \"nn_baseline\",\n",
    "                                        n_layers=self.n_layers,\n",
    "                                        size=self.size\n",
    "                                    ))\n",
    "            self.sy_target_n = tf.placeholder(dtype=tf.float32, shape=[None], name='reward_label')\n",
    "            self.baseline_loss =tf.losses.mean_squared_error(self.sy_target_n, self.baseline_prediction, scope='nn_baseline_loss')\n",
    "            self.baseline_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.baseline_loss)\n",
    "            \n",
    "        \n",
    "    def sample_trajectories(self, itr, env):\n",
    "        \"\"\"\n",
    "        Collect paths until we have enough timesteps.\n",
    "        \"\"\"\n",
    "        timesteps_this_batch = 0\n",
    "        paths = []\n",
    "        \n",
    "        while True:\n",
    "            animate_this_episode = (len(paths) == 0 and (itr % 10 == 0) and self.animate)\n",
    "            path = self.sample_trajectory(env, animate_this_episode)\n",
    "            paths.append(path)\n",
    "            timesteps_this_batch += pathlength(path)\n",
    "            if timesteps_this_batch > self.min_timsteps_per_batch:\n",
    "                break\n",
    "        return paths, timesteps_this_batch\n",
    "            \n",
    "            \n",
    "    def sample_trajectory(self, env, animate_this_episode):\n",
    "        \n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards = [], [], []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "            obs.append(ob)\n",
    "            \n",
    "            ac, policy_parameters = self.sess.run([self.sy_sampled_ac, self.policy_parameters], feed_dict={self.sy_ob_no: ob[None, :]})\n",
    "            ac = ac[0]\n",
    "            acs.append(ac)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            \n",
    "            if done or steps > self.max_length_path:\n",
    "                break\n",
    "                \n",
    "        path = {\n",
    "            \"observation\": np.array(obs, dtype=np.float32),\n",
    "            \"reward\": np.array(rewards, dtype=np.float32),\n",
    "            \"action\": np.array(acs, dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def sum_of_rewards(self, re_n):\n",
    "        \"\"\"\n",
    "        Monte Carlo estimation of the Q function.\n",
    "        \n",
    "        arguments:\n",
    "            re_n: lenghth: num_paths, Each element in re_n is numpy array containing the rewards for particular path\n",
    "            \n",
    "        returns:\n",
    "            q_n: shape: (sum_of_path_lengths): A single vector of estimated Q values whole length is the sum of the\n",
    "                lengths of the paths.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_n = []\n",
    "        if self.reward_to_go:\n",
    "            for path_i in re_n:\n",
    "                reversed_path = np.flip(path_i)\n",
    "                reward_i = []\n",
    "                curr_reward = 0\n",
    "                for r in reversed_path:\n",
    "                    curr_reward = curr_reward * self.gamma + r\n",
    "                    reward_i.append(curr_reward)\n",
    "                reward_i.reverse()\n",
    "                q_n += reward_i\n",
    "        else: # At every timestep, gradient is weighted by the full reward.\n",
    "            for path_i in re_n:\n",
    "                reversed_path = np.flip(path_i)\n",
    "                reward_i = []\n",
    "                curr_reward = 0\n",
    "                for r in reversed_path:\n",
    "                    curr_reward = curr_reward * self.gamma + r\n",
    "                reward_i = [curr_reward for i in range(len(path_i))]\n",
    "                q_n += reward_i\n",
    "        return np.array(q_n)\n",
    "    \n",
    "    def compute_advantage(self, ob_no, q_n):\n",
    "        \"\"\"\n",
    "        Compute advantages by possibly subtracting a baseline from estimated q values.\n",
    "        arguments: \n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            q_n: shape: (sum_of_path_lengths), A single vector of estimated q values.\n",
    "                \n",
    "        returns:\n",
    "            adv_n: shape: (sum_of_path_lengths), A single vector of the estimated advantages whose lenght is thes sum of length\n",
    "                    of paths\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.nn_baseline: # basline estimated with NN\n",
    "            b_n = self.sess.run(self.baseline_prediction, feed_dict={self.sy_ob_no: ob_no})\n",
    "            b_n = (b_n - b_n.mean()) / (b_n.std() + EPSILON) * q_n.std() + q_n.mean() # rescale b_n to match the statistics of q_n\n",
    "            adv_n = q_n - b_n\n",
    "        else:\n",
    "            adv_n = q_n.copy()\n",
    "        return adv_n\n",
    "    \n",
    "    def estimate_returns(self, ob_no, re_n):\n",
    "        \"\"\"\n",
    "        Estimate the returns over a set of trajectories.\n",
    "        \n",
    "        arguments: \n",
    "            ob_no: shape: (sum_of_path_length, ob_dim)\n",
    "            re_n: length: num_paths, each element is a numpy array containing rewards of a paricular path.\n",
    "            \n",
    "        returns:\n",
    "            q_n: shape: (sum_of_path_lengths), A single vector of estimated Q values\n",
    "            adv_n: shape: (sum_of_path_lengths),  A single vector of estimated Advantages.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_n = self.sum_of_rewards(re_n)\n",
    "        adv_n = self.compute_advantage(ob_no, q_n)\n",
    "        \n",
    "        # normalize the advantages to reduce the variance.\n",
    "        if self.normalize_advantages:\n",
    "            adv_n = (adv_n - adv_n.mean()) / (adv_n.std() + EPSILON)\n",
    "            \n",
    "        return q_n, adv_n\n",
    "    \n",
    "    def update_parameters(self, ob_no, ac_na, q_n, adv_n):\n",
    "        \"\"\"\n",
    "        Update the parameters of the policy and (possibly) the neural network baseline.\n",
    "        \n",
    "        argumenst: \n",
    "            ob_no: (sum_of_path_lengths, ob_dim)\n",
    "            ac_na: (sum_of_path_lengths)\n",
    "            q_n: (sum_of_path_lengths)\n",
    "            adv_n: (sum_of_path_lengths)\n",
    "            \n",
    "        returns:\n",
    "            nothing\n",
    "        \"\"\"\n",
    "        \n",
    "        # Optimising NN baseling\n",
    "        if self.nn_baseline:\n",
    "            target_n = (q_n - q_n.mean()) / (q_n.std() + EPSILON)\n",
    "            _, target_loss = self.sess.run([self.baseline_update_op, self.baseline_loss], feed_dict={self.sy_ob_no: ob_no, self.sy_target_n: target_n})\n",
    "            \n",
    "        # Policy Update\n",
    "        _, loss, policy_parameters, logprob_n = self.sess.run([self.update_op, self.sy_loss, self.policy_parameters, self.sy_logprob_n],\n",
    "                                                             feed_dict={self.sy_ob_no: ob_no, self.sy_ac_na: ac_na, self.sy_adv_n: adv_n})\n",
    "        return loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make the Gym Environment\n",
    "* Initialize the observation and action shapes.\n",
    "* Initialize computation graph params ( for setting up the training op ), sample_params ( for sampling trajectories) and estimate_return params ( for estimating rewards and advantages of sampled episodes)\n",
    "* Initialze the Agent with those params\n",
    "* Build the computation graph of the agent.\n",
    "    * Initialize the place holders for observations(inputs), actions(loss) and advantage(loss)\n",
    "    * Build the forward propagation of the policy - build the MLP - returns logits.\n",
    "    * Add the graph for sampling action from those logits- Discrete and Continous.\n",
    "    * Add the graph for getting the log probability between the logits and sampled action - used in loss.\n",
    "    * Add the graph for log probability weighted advantage.\n",
    "    * Add the graph for reduce mean of above - loss function.\n",
    "    * Add the graph of update op, AdamOptimizer\n",
    "    * Incase of Baseline Neural Network.\n",
    "        * Build separate graph for baseline estimation\n",
    "        * Create a place holder for reward label\n",
    "        * Add the graph for loss function\n",
    "        * Add the grapg for update op\n",
    "* Initialize Tensorflow Session\n",
    "* for iterations:\n",
    "    * sample multiple trajectories/paths\n",
    "        * Till we have the desired batchsize of obs, acs, and rewards.\n",
    "            * sample paths\n",
    "                * env.reset()\n",
    "                * sample action by running the sample_ac graph\n",
    "                * env.step()\n",
    "                * collect reward\n",
    "    * Stack sampled paths' obs and acs on top of another. rewards still list of lists(cause we need to calculate discounted sum of rewards which depends on the episode)\n",
    "    * Estimate the returns. using observations and rewards from sampling\n",
    "        * q_n : discounted sum of rewards\n",
    "        * adv_n: advantage. q_n - b_n : b_n with a NN ( obs is used here to pass to the network)\n",
    "    * Update parameters of Policy and NN Baseline\n",
    "        * for baseline NN, target is q_n: session run baseline_update_op\n",
    "        * for policy, session run update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PG(exp_name, env_name, n_iter, gamma, min_timesteps_per_batch, max_path_length, learning_rate, reward_to_go,\n",
    "             animate, logdir, normalize_advantages, nn_baseline, seed, n_layers, size):\n",
    "    \"\"\"\n",
    "    Train the PG agent.\n",
    "    arguments:\n",
    "        exp_name: name of the experiment\n",
    "        env_name: name of the gym env\n",
    "        n_iter: epochs to train\n",
    "        gamma: discount factor\n",
    "        min_timesteps_per_batch: how many steps in a batch of training data.\n",
    "        max_path_length: max path length of a trajectory/episode\n",
    "        learning_rate: \n",
    "        reward_to_go: whether to use reward_to_go or full_reward when doing MC evaluation: True/False\n",
    "        animate: Render env or not, True/False\n",
    "        logdir: directory to store logs\n",
    "        normalize_advantages: whether to normalize the advantanges calculated.\n",
    "        nn_baseline: whether to use a NN for baseline estimation\n",
    "        seed : random seet\n",
    "        n_layers : number of layers in the MLP\n",
    "        size : number neurons in the hidden layer of MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Set the random seed\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    # Maximum lenth of episodes.\n",
    "    max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "    \n",
    "    # Is this env continous or discrete?\n",
    "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    \n",
    "    # Observation and action sizes.\n",
    "    ob_dim = env.observation_space.shape[0]\n",
    "    print(\"Observation Shape: \", env.observation_space.shape)\n",
    "    ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "    print(\"Action Shape: \", env.action_space.n)\n",
    "    \n",
    "    \n",
    "    # Initializing the agent\n",
    "    computation_graph_args = {\n",
    "        'n_layer': n_layer,\n",
    "        'ob_dim' : ob_dim,\n",
    "        'ac_dim' : ac_dim,\n",
    "        'discrete' : discrete,\n",
    "        'size' : size,\n",
    "        'learning_rate' : learning_rate,\n",
    "    }\n",
    "    \n",
    "    sample_trajectory_args={\n",
    "        'animate': animate,\n",
    "        'max_path_length': max_path_length,\n",
    "        'min_timesteps_per_batch' : min_timesteps_per_batch,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    estimate_return_args = {\n",
    "        'gamma': gamma,\n",
    "        'reward_to_go': reward_to_go,\n",
    "        'nn_baseline' : nn_baseline,\n",
    "        'normalize_advantages' : normalize_advantages\n",
    "    }\n",
    "    \n",
    "    agent = Agent(computation_graph_args, sample_trajectory_args, estimate_return_args)\n",
    "    \n",
    "    # build the computation grapg\n",
    "    agent.build_computation_graph()\n",
    "    \n",
    "    # tensorflow config, session, variable initialization.\n",
    "    agent.init_tf_sess()\n",
    "    \n",
    "    \n",
    "    # Training Loop\n",
    "    \n",
    "    total_timesteps = 0\n",
    "    for itr in range(n_iter):\n",
    "        print(\"************** Iteration %d ******************\" % itr)\n",
    "        # collect experiences.\n",
    "        paths, timesteps_this_batch = agent.sample_trajectories(itr, env)\n",
    "        total_timesteps += timesteps_this_batch\n",
    "        \n",
    "        # Build arrays of observation, action for the policy gradient update by concatenating accross paths.\n",
    "        # put all data into a big tensor.\n",
    "        ob_no = np.concatenate([path['observation'] for path in paths])\n",
    "        ac_na = np.concatenate([path['action'] for path in paths])\n",
    "        re_n = [path['reward'] for path in paths]\n",
    "        print(\"Observation\")\n",
    "        pprint(ob_no.shape)\n",
    "        print(\"Actions\")\n",
    "        pprint(ac_na.shape)\n",
    "        print(\"Rewrads\")\n",
    "        pprint(len(re_n))\n",
    "        \n",
    "        # estimate q and advantage\n",
    "        q_n, adv_n = agent.estimate_returns(ob_no, re_n)\n",
    "        \n",
    "        # update parameters of policy, nn baseline\n",
    "        agent.update_parameters(ob_no, ac_na, q_n, adv_n)\n",
    "        \n",
    "        # log stats\n",
    "        returns = [path['reward'].sum() for path in paths]\n",
    "        ep_lengths = [pathlength(path) for path in paths]\n",
    "        print(\"Time: \", time.time() - start)\n",
    "        print(\"Iteration: ,\", itr)\n",
    "        print(\"Average Return: \", np.mean(returns))\n",
    "        print(\"Std Return: \", np.std(returns))\n",
    "        print(\"Max Return: \", np.max(returns))\n",
    "        print(\"MinReturn: \", np.min(returns))\n",
    "        print(\"EplenMean: ,\", np.mean(ep_lengths))\n",
    "        print(\"EplenStd: ,\", np.std(ep_lengths))\n",
    "        print(\"TimeStepsThisBatch: \", timesteps_this_batch)\n",
    "        print(\"TimeStepsSoFar: \", total_timesteps)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"test\"\n",
    "env_name = 'CartPole-v0'\n",
    "render = True\n",
    "gamma = 0.99\n",
    "n_iter = 20\n",
    "batch_size = 1000\n",
    "ep_len = -1\n",
    "learning_rate = 5e-3\n",
    "reward_to_go = True\n",
    "normalize_advantages = True\n",
    "nn_baseline = True\n",
    "seed = 42\n",
    "n_experiments = 1\n",
    "n_layer = 2\n",
    "size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment with seed:  62\n",
      "Observation Shape:  (4,)\n",
      "Action Shape:  2\n",
      "************** Iteration 0 ******************\n",
      "Observation\n",
      "(1006, 4)\n",
      "Actions\n",
      "(1006,)\n",
      "Rewrads\n",
      "49\n",
      "Time:  5.984406232833862\n",
      "Iteration: , 0\n",
      "Average Return:  20.530613\n",
      "Std Return:  8.86659\n",
      "Max Return:  57.0\n",
      "MinReturn:  9.0\n",
      "EplenMean: , 20.53061224489796\n",
      "EplenStd: , 8.866589526375844\n",
      "TimeStepsThisBatch:  1006\n",
      "TimeStepsSoFar:  1006\n",
      "************** Iteration 1 ******************\n",
      "Observation\n",
      "(1017, 4)\n",
      "Actions\n",
      "(1017,)\n",
      "Rewrads\n",
      "39\n",
      "Time:  6.835920333862305\n",
      "Iteration: , 1\n",
      "Average Return:  26.076923\n",
      "Std Return:  13.278833\n",
      "Max Return:  69.0\n",
      "MinReturn:  11.0\n",
      "EplenMean: , 26.076923076923077\n",
      "EplenStd: , 13.278833389028193\n",
      "TimeStepsThisBatch:  1017\n",
      "TimeStepsSoFar:  2023\n",
      "************** Iteration 2 ******************\n",
      "Observation\n",
      "(1004, 4)\n",
      "Actions\n",
      "(1004,)\n",
      "Rewrads\n",
      "36\n",
      "Time:  7.509249448776245\n",
      "Iteration: , 2\n",
      "Average Return:  27.88889\n",
      "Std Return:  11.034738\n",
      "Max Return:  54.0\n",
      "MinReturn:  10.0\n",
      "EplenMean: , 27.88888888888889\n",
      "EplenStd: , 11.034737518344759\n",
      "TimeStepsThisBatch:  1004\n",
      "TimeStepsSoFar:  3027\n",
      "************** Iteration 3 ******************\n",
      "Observation\n",
      "(1026, 4)\n",
      "Actions\n",
      "(1026,)\n",
      "Rewrads\n",
      "25\n",
      "Time:  8.339380264282227\n",
      "Iteration: , 3\n",
      "Average Return:  41.04\n",
      "Std Return:  20.064854\n",
      "Max Return:  101.0\n",
      "MinReturn:  11.0\n",
      "EplenMean: , 41.04\n",
      "EplenStd: , 20.064854846223035\n",
      "TimeStepsThisBatch:  1026\n",
      "TimeStepsSoFar:  4053\n",
      "************** Iteration 4 ******************\n",
      "Observation\n",
      "(1024, 4)\n",
      "Actions\n",
      "(1024,)\n",
      "Rewrads\n",
      "27\n",
      "Time:  9.189095973968506\n",
      "Iteration: , 4\n",
      "Average Return:  37.925926\n",
      "Std Return:  21.957747\n",
      "Max Return:  104.0\n",
      "MinReturn:  10.0\n",
      "EplenMean: , 37.925925925925924\n",
      "EplenStd: , 21.957747179064118\n",
      "TimeStepsThisBatch:  1024\n",
      "TimeStepsSoFar:  5077\n",
      "************** Iteration 5 ******************\n",
      "Observation\n",
      "(1046, 4)\n",
      "Actions\n",
      "(1046,)\n",
      "Rewrads\n",
      "26\n",
      "Time:  10.079654932022095\n",
      "Iteration: , 5\n",
      "Average Return:  40.23077\n",
      "Std Return:  18.643019\n",
      "Max Return:  91.0\n",
      "MinReturn:  16.0\n",
      "EplenMean: , 40.23076923076923\n",
      "EplenStd: , 18.643018268979805\n",
      "TimeStepsThisBatch:  1046\n",
      "TimeStepsSoFar:  6123\n",
      "************** Iteration 6 ******************\n",
      "Observation\n",
      "(1040, 4)\n",
      "Actions\n",
      "(1040,)\n",
      "Rewrads\n",
      "19\n",
      "Time:  11.008798360824585\n",
      "Iteration: , 6\n",
      "Average Return:  54.736843\n",
      "Std Return:  27.266409\n",
      "Max Return:  110.0\n",
      "MinReturn:  16.0\n",
      "EplenMean: , 54.73684210526316\n",
      "EplenStd: , 27.26640907255503\n",
      "TimeStepsThisBatch:  1040\n",
      "TimeStepsSoFar:  7163\n",
      "************** Iteration 7 ******************\n",
      "Observation\n",
      "(1022, 4)\n",
      "Actions\n",
      "(1022,)\n",
      "Rewrads\n",
      "17\n",
      "Time:  11.888221025466919\n",
      "Iteration: , 7\n",
      "Average Return:  60.117645\n",
      "Std Return:  33.47498\n",
      "Max Return:  130.0\n",
      "MinReturn:  21.0\n",
      "EplenMean: , 60.11764705882353\n",
      "EplenStd: , 33.474981620064675\n",
      "TimeStepsThisBatch:  1022\n",
      "TimeStepsSoFar:  8185\n",
      "************** Iteration 8 ******************\n",
      "Observation\n",
      "(1011, 4)\n",
      "Actions\n",
      "(1011,)\n",
      "Rewrads\n",
      "12\n",
      "Time:  12.793153047561646\n",
      "Iteration: , 8\n",
      "Average Return:  84.25\n",
      "Std Return:  57.593582\n",
      "Max Return:  200.0\n",
      "MinReturn:  30.0\n",
      "EplenMean: , 84.25\n",
      "EplenStd: , 57.59358326526778\n",
      "TimeStepsThisBatch:  1011\n",
      "TimeStepsSoFar:  9196\n",
      "************** Iteration 9 ******************\n",
      "Observation\n",
      "(1136, 4)\n",
      "Actions\n",
      "(1136,)\n",
      "Rewrads\n",
      "11\n",
      "Time:  13.849955558776855\n",
      "Iteration: , 9\n",
      "Average Return:  103.27273\n",
      "Std Return:  53.564053\n",
      "Max Return:  193.0\n",
      "MinReturn:  38.0\n",
      "EplenMean: , 103.27272727272727\n",
      "EplenStd: , 53.56404986571245\n",
      "TimeStepsThisBatch:  1136\n",
      "TimeStepsSoFar:  10332\n",
      "************** Iteration 10 ******************\n",
      "Observation\n",
      "(1052, 4)\n",
      "Actions\n",
      "(1052,)\n",
      "Rewrads\n",
      "12\n",
      "Time:  21.49416995048523\n",
      "Iteration: , 10\n",
      "Average Return:  87.666664\n",
      "Std Return:  36.279774\n",
      "Max Return:  139.0\n",
      "MinReturn:  31.0\n",
      "EplenMean: , 87.66666666666667\n",
      "EplenStd: , 36.279777042068794\n",
      "TimeStepsThisBatch:  1052\n",
      "TimeStepsSoFar:  11384\n",
      "************** Iteration 11 ******************\n",
      "Observation\n",
      "(1069, 4)\n",
      "Actions\n",
      "(1069,)\n",
      "Rewrads\n",
      "9\n",
      "Time:  22.440802335739136\n",
      "Iteration: , 11\n",
      "Average Return:  118.77778\n",
      "Std Return:  56.721107\n",
      "Max Return:  200.0\n",
      "MinReturn:  34.0\n",
      "EplenMean: , 118.77777777777777\n",
      "EplenStd: , 56.72110674711208\n",
      "TimeStepsThisBatch:  1069\n",
      "TimeStepsSoFar:  12453\n",
      "************** Iteration 12 ******************\n",
      "Observation\n",
      "(1100, 4)\n",
      "Actions\n",
      "(1100,)\n",
      "Rewrads\n",
      "9\n",
      "Time:  23.427438020706177\n",
      "Iteration: , 12\n",
      "Average Return:  122.22222\n",
      "Std Return:  39.826782\n",
      "Max Return:  194.0\n",
      "MinReturn:  65.0\n",
      "EplenMean: , 122.22222222222223\n",
      "EplenStd: , 39.82678545283529\n",
      "TimeStepsThisBatch:  1100\n",
      "TimeStepsSoFar:  13553\n",
      "************** Iteration 13 ******************\n",
      "Observation\n",
      "(1020, 4)\n",
      "Actions\n",
      "(1020,)\n",
      "Rewrads\n",
      "7\n",
      "Time:  24.352747678756714\n",
      "Iteration: , 13\n",
      "Average Return:  145.71428\n",
      "Std Return:  41.808113\n",
      "Max Return:  200.0\n",
      "MinReturn:  73.0\n",
      "EplenMean: , 145.71428571428572\n",
      "EplenStd: , 41.80811365449222\n",
      "TimeStepsThisBatch:  1020\n",
      "TimeStepsSoFar:  14573\n",
      "************** Iteration 14 ******************\n",
      "Observation\n",
      "(1069, 4)\n",
      "Actions\n",
      "(1069,)\n",
      "Rewrads\n",
      "7\n",
      "Time:  25.56950283050537\n",
      "Iteration: , 14\n",
      "Average Return:  152.71428\n",
      "Std Return:  53.073303\n",
      "Max Return:  200.0\n",
      "MinReturn:  72.0\n",
      "EplenMean: , 152.71428571428572\n",
      "EplenStd: , 53.07330317781325\n",
      "TimeStepsThisBatch:  1069\n",
      "TimeStepsSoFar:  15642\n",
      "************** Iteration 15 ******************\n",
      "Observation\n",
      "(1086, 4)\n",
      "Actions\n",
      "(1086,)\n",
      "Rewrads\n",
      "7\n",
      "Time:  26.436220169067383\n",
      "Iteration: , 15\n",
      "Average Return:  155.14285\n",
      "Std Return:  55.385475\n",
      "Max Return:  200.0\n",
      "MinReturn:  35.0\n",
      "EplenMean: , 155.14285714285714\n",
      "EplenStd: , 55.385476619851914\n",
      "TimeStepsThisBatch:  1086\n",
      "TimeStepsSoFar:  16728\n",
      "************** Iteration 16 ******************\n",
      "Observation\n",
      "(1122, 4)\n",
      "Actions\n",
      "(1122,)\n",
      "Rewrads\n",
      "6\n",
      "Time:  27.304830312728882\n",
      "Iteration: , 16\n",
      "Average Return:  187.0\n",
      "Std Return:  29.068884\n",
      "Max Return:  200.0\n",
      "MinReturn:  122.0\n",
      "EplenMean: , 187.0\n",
      "EplenStd: , 29.068883707497267\n",
      "TimeStepsThisBatch:  1122\n",
      "TimeStepsSoFar:  17850\n",
      "************** Iteration 17 ******************\n",
      "Observation\n",
      "(1051, 4)\n",
      "Actions\n",
      "(1051,)\n",
      "Rewrads\n",
      "6\n",
      "Time:  28.131463289260864\n",
      "Iteration: , 17\n",
      "Average Return:  175.16667\n",
      "Std Return:  38.080692\n",
      "Max Return:  200.0\n",
      "MinReturn:  100.0\n",
      "EplenMean: , 175.16666666666666\n",
      "EplenStd: , 38.08068918610704\n",
      "TimeStepsThisBatch:  1051\n",
      "TimeStepsSoFar:  18901\n",
      "************** Iteration 18 ******************\n",
      "Observation\n",
      "(1084, 4)\n",
      "Actions\n",
      "(1084,)\n",
      "Rewrads\n",
      "6\n",
      "Time:  29.060964345932007\n",
      "Iteration: , 18\n",
      "Average Return:  180.66667\n",
      "Std Return:  43.230644\n",
      "Max Return:  200.0\n",
      "MinReturn:  84.0\n",
      "EplenMean: , 180.66666666666666\n",
      "EplenStd: , 43.23064756499593\n",
      "TimeStepsThisBatch:  1084\n",
      "TimeStepsSoFar:  19985\n",
      "************** Iteration 19 ******************\n",
      "Observation\n",
      "(1095, 4)\n",
      "Actions\n",
      "(1095,)\n",
      "Rewrads\n",
      "6\n",
      "Time:  29.953675985336304\n",
      "Iteration: , 19\n",
      "Average Return:  182.5\n",
      "Std Return:  39.13119\n",
      "Max Return:  200.0\n",
      "MinReturn:  95.0\n",
      "EplenMean: , 182.5\n",
      "EplenStd: , 39.131189606246316\n",
      "TimeStepsThisBatch:  1095\n",
      "TimeStepsSoFar:  21080\n"
     ]
    }
   ],
   "source": [
    "processes = []\n",
    "for i in range(n_experiments):\n",
    "    \n",
    "    seed = seed + 10\n",
    "    print(\"Running Experiment with seed: \", seed)\n",
    "    \n",
    "    def train_func():\n",
    "        train_PG(\n",
    "            exp_name=exp_name,\n",
    "            env_name=env_name,\n",
    "            n_iter=n_iter,\n",
    "            gamma=gamma,\n",
    "            min_timesteps_per_batch=batch_size,\n",
    "            max_path_length=None,\n",
    "            learning_rate=learning_rate,\n",
    "            reward_to_go=reward_to_go,\n",
    "            animate=render,\n",
    "            logdir=None,\n",
    "            normalize_advantages=normalize_advantages,\n",
    "            nn_baseline=nn_baseline,\n",
    "            seed=seed,\n",
    "            n_layers=n_layer,\n",
    "            size=size\n",
    "        )\n",
    "        \n",
    "    p = Process(target=train_func, args=tuple())\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    \n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
