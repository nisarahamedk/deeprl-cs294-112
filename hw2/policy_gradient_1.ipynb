{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"test\"\n",
    "env_name = \"CartPole-v0\"\n",
    "n_iter = 1\n",
    "reward_to_go = True\n",
    "nn_baseline = True\n",
    "normalize_advantages = True\n",
    "n_layers = 2\n",
    "size = 64\n",
    "render = True\n",
    "gamma = 0.99\n",
    "seed = 42\n",
    "learning_rate = 1e-4\n",
    "batch_size=1000\n",
    "max_path_length = None\n",
    "n_experiments = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, computation_graph_params, sample_trajectories_params, estimate_returns_params):\n",
    "        \n",
    "        # computation graph params\n",
    "        self.ob_dim = computation_graph_params['ob_dim']\n",
    "        self.ac_dim = computation_graph_params['ac_dim']\n",
    "        self.n_layers = computation_graph_params['n_layers']\n",
    "        self.size = computation_graph_params['size']\n",
    "        self.learning_rate = computation_graph_params['learning_rate']\n",
    "        self.is_discrete = computation_graph_params['discrete']\n",
    "        \n",
    "        #  Sample trajectory params\n",
    "        self.max_path_length = sample_trajectories_params['max_path_lenght']\n",
    "        self.timesteps_per_batch = sample_trajectories_params['timesteps_per_batch']\n",
    "        self.animate = sample_trajectories_params['animate']\n",
    "        \n",
    "        # Estimate return params\n",
    "        self.reward_to_go = estimate_returns_params['reward_to_go']\n",
    "        self.nn_baseline = estimate_returns_params['nn_baseline']\n",
    "        self.normalize_advatantages = estimate_returns_params['normalize_advantages']\n",
    "        self.gamma = estimate_returns_params['gamma']\n",
    "        \n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "        define placeholders for the inputs such as observations, actions(in loss), advantage(in loss)\n",
    "        \"\"\"\n",
    "        # observation\n",
    "        sy_ob_no = tf.placeholder(dtype=tf.float32, shape=[None, self.ob_dim], name='observation')\n",
    "        \n",
    "        # action\n",
    "        if self.is_discrete:\n",
    "            sy_ac_na = tf.placeholder(dtype=tf.int32, shape=[None], name='action')\n",
    "        else:\n",
    "            sy_ac_na = tf.placeholder(dtype=tf.float32, shape=[None, self.ac_dim], name='actions')\n",
    "            \n",
    "        # advantage\n",
    "        sy_adv_n = tf.placeholder(dtype=tf.float32, shape=[None], name='advantage')\n",
    "        \n",
    "    def build_mlp(self, input_placeholder, output_size, variable_scope, n_layers=2, hidden_size=64, activation=None, \n",
    "                  output_activation=None):\n",
    "        \"\"\"\n",
    "        build the neural network graph\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(variable_scope):\n",
    "            layer = input_placeholder\n",
    "            for i in range(n_layers):\n",
    "                layer = tf.layers.dense(layer, hidden_size, activation=activation)\n",
    "\n",
    "            output_placeholder = tf.layers.dense(layer, output_size, activation=None)\n",
    "            \n",
    "        return output_placeholder\n",
    "            \n",
    "        \n",
    "    def policy_forward(self, sy_ob_no):\n",
    "        variable_scope = 'nn_policy'\n",
    "        if self.is_discrete:\n",
    "            sy_logits_na = self.build_mlp(input_placeholder=sy_ob_no, output_size=self.ac_dim, variable_scope=variable_scope, \n",
    "                                     n_layers=self.n_layers, hidden_size=self.size, activation=tf.nn.relu, \n",
    "                                     output_activation=None)\n",
    "            return sy_logits_na\n",
    "        else: # mean and std of guassian\n",
    "            sy_mean = self.build_mlp(input_placeholder=sy_ob_no, output_size=self.ac_dim, variable_scope=variable_scope,\n",
    "                                n_layers=self.n_layers, hidden_size=self.size, activation=tf.nn.relu, \n",
    "                                output_activation=None)\n",
    "            sy_std = tf.Variable(tf.zeros(self.ac_dim), name='sy_logstd')\n",
    "            return sy_mean, sy_std\n",
    "        \n",
    "    def sample_action(self, policy_parameters):\n",
    "        \"\"\"\n",
    "        graph for sampling action given logits\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('sampled_action'):\n",
    "            \n",
    "            if self.is_discrete:\n",
    "                sy_sampled_ac = tf.squeeze(tf.multinomial(policy_parameters, 1), axis=1)\n",
    "            else:\n",
    "                sy_mean, sy_logstd = policy_parameters\n",
    "                sy_sampled_ac = sy_mean + tf.exp(sy_logstd) * tf.random_normal(shape=tf.shape(sy_mean))\n",
    "                \n",
    "            return sy_sampled_ac\n",
    "        \n",
    "    def get_logprob(self, policy_parameters, sy_ac_na):\n",
    "        \"\"\"\n",
    "        graph for computing the log probabilitie between the logits and actions taken.\n",
    "        \n",
    "        arguments: \n",
    "            policy_parameters: \n",
    "                if is_discrete:\n",
    "                    sy_logits_na: shape(batch_size, self.ac_dim)\n",
    "                else:\n",
    "                    sy_mean_na : shape: (batch_size, self.ac_dim)\n",
    "                    sy_logstd_n : shape: (self.ac_dim, )\n",
    "            sy_ac_na:\n",
    "                if is_discrete:\n",
    "                    shape : (batch_size,)\n",
    "                else:\n",
    "                    shape : (batch_size, self.ac_dim)\n",
    "        returns:\n",
    "            sy_logprob_n: shape : (batch_size, )\n",
    "        \"\"\"\n",
    "        if self.is_discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            sy_logprobs_n = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sy_logits_na, labels=sy_ac_na)\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            probabilities = tf.distribution.Normal(sy_mean, tf.exp(sy_logstd)).prob(sy_ac_na)\n",
    "            sy_logprob_n = tf.log(tf.reduce_prod(probabilities, axis=1))\n",
    "            \n",
    "        return sy_logprob_n\n",
    "        \n",
    "        \n",
    "    def build_computation_graph(self):\n",
    "        \"\"\"\n",
    "        build the computation graph of the training\n",
    "        \"\"\"\n",
    "        \n",
    "        # define input placeholders. \n",
    "        self.sy_ob_no, self.sy_ac_na, self.sy_adv_n = self.define_placeholders()\n",
    "        \n",
    "        # build the graph forward pass through the policy\n",
    "        self.policy_parameters = self.policy_forward(self.sy_ob_no)\n",
    "        \n",
    "        # add graph for sampling actions from logits.\n",
    "        self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "        \n",
    "        # add graph for getting log probability of actions and logits.\n",
    "        self.sy_logprob_n = self.get_logprob(self.policy_parameters, self.sy_ac_na)\n",
    "        \n",
    "        # add graph for logprobabilty with weighted ad\n",
    "        with tf.variable_scope(\"log_prob_with_weighted_adv\"):\n",
    "            sy_weighted_logprob_n = tf.multiply(self.sy_logprob_n, self.adv_n)\n",
    "            \n",
    "        # add grapg for the loss function.\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.sy_loss = tf.reduce_mean(sy_weighted_logprob_n)\n",
    "            \n",
    "        # optimizer.\n",
    "        self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.sy_loss)\n",
    "        \n",
    "        # in case of nn baseline\n",
    "        if self.nn_baseline:\n",
    "            self.baseline_predictions = tf.squeeze(build_mlp(self.sy_ob_no, 1, 'nn_baseline', \n",
    "                                                      n_layers=self.n_layers, size=self.size,\n",
    "                                                      activatoin=tf.nn.relu))\n",
    "            self.sy_target_n = tf.placeholder(dtype=tf.float32, shape=[None], name='reward_label')\n",
    "            self.baseline_loss = tf.losses.mean_squared_error(self.sy_target_n, self.baseline_predictions, scope='nn_baseline_loss')\n",
    "            self.baseline_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.baseline_loss)\n",
    "            \n",
    "    def init_tf_sess(self):\n",
    "        \"\"\"\n",
    "        initialize tensorflow session\n",
    "        \"\"\"\n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__()\n",
    "        tf.global_variable_initializer()\n",
    "        \n",
    "    def batchlength(self, path):\n",
    "        return len(path['rewards'])\n",
    "    \n",
    "    def sample_trajectories(self, iteration, env):\n",
    "        \"\"\"\n",
    "        sample a set of trjectories\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        while True:\n",
    "            animate_this_episode = len(paths) == 0 and (iteration % 10 == 0) and self.animate\n",
    "            path = self.sample_trajectory(env, animate_this_episode)\n",
    "            paths.append(path)\n",
    "            timesteps_this_batch += self.batchlength(path)\n",
    "            if timesteps_this_batch > self.timesteps_per_batch:\n",
    "                break\n",
    "        return paths, timesteps_this_batch\n",
    "            \n",
    "    def sample_trajectory(self, env, animate):\n",
    "        \"\"\"\n",
    "        sample a trajectory/episode from the environment\n",
    "        \"\"\"\n",
    "        ob = env.reset()\n",
    "        path = dict()\n",
    "        obs, acs, rewards = [], [], []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "            obs.append(ob)\n",
    "            ac = self.sess.run([self.sy_sampled_ac], feed_dict={self.sy_ob_no: ob})\n",
    "            ac = ac[0]\n",
    "            acs.append(ac)\n",
    "            obs, reward, done, _ = env.step(ac)\n",
    "            rewards.append(reward)\n",
    "            steps += 1\n",
    "            \n",
    "            if done or steps > self.max_path_length:\n",
    "                break\n",
    "                \n",
    "        path['observation'] = np.array(obs, dtype=np.float32)\n",
    "        path['actions'] = np.array(acs, dtype=np.int32)\n",
    "        path['rewards'] = np.array(rewards, dtype=np.int32)\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    \n",
    "    def estimate_returns(self, ob_no, re_n):\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PG(exp_name, env_name, n_iter, n_layers, size, learning_rate, batch_size, render,\n",
    "             reward_to_go, nn_baseline, normalize_advantages, gamma, seed, max_path_length):\n",
    "    \n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # set random seeds\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.set_random_seed(seed)\n",
    "    \n",
    "    is_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    ac_dim = env.action_space.n if is_discrete else env.action_space.shape[0]\n",
    "    \n",
    "    max_path_length = max_path_length if max_path_length else env.spec.max_episode_steps\n",
    "    \n",
    "    # initializing the parameters for the agent\n",
    "    computation_graph_params = {\n",
    "        'n_layers': n_layers,\n",
    "        'size' : size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'ob_dim' : obs_dim,\n",
    "        'ac_dim' : ac_dim,\n",
    "        'discrete' : is_discrete,\n",
    "    }\n",
    "    \n",
    "    sample_trajectory_params = {\n",
    "        'timesteps_per_batch' : batch_size,\n",
    "        'max_path_length': max_path_length,\n",
    "        'animate' : render\n",
    "    }\n",
    "    \n",
    "    estimate_return_params = {\n",
    "        'gamma' : gamma,\n",
    "        'nn_baseline' : nn_baseline,\n",
    "        'reward_to_go' : reward_to_go,\n",
    "        'normalize_advantages' : normalize_advantages\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # initialize the agent\n",
    "    agent = Agent(computation_graph_params, sample_trajectory_params, estimate_return_params)\n",
    "    \n",
    "    # build the computation graph of the agent training.\n",
    "    agent.build_computation_graph()\n",
    "    \n",
    "    # init tensorflow session.\n",
    "    agent.init_tf_sess()\n",
    "    \n",
    "    for iteration in range(n_iter):\n",
    "        print(\"************* Iteration %d ***************\", iteration)\n",
    "        paths, timesteps_this_batch = agent.sample_trajectories(iteration, env)\n",
    "        total_timesteps += timesteps_this_batch\n",
    "        \n",
    "        # stack observations, actions into a 1D array.\n",
    "        # Rewards are not stacked yet, what we have is the immediate reward, we need to calculate the estimated return which\n",
    "        # is episode specific.\n",
    "        ac_na = np.concatenate([path['action'] for path in paths])\n",
    "        ob_no = np.concatenate([path['observation'] for path in paths])\n",
    "        re_n = [path['reward'] for path in paths] # not stacked yet.\n",
    "        \n",
    "        # calculate q_n and adv_n for each state and action. Here we stack it into a single 1D array\n",
    "        q_n, adv_n = agent.estimate_returns(ob_no, re_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
